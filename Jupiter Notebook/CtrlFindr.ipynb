{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contact contact {at} scartozzi [dot] eu if you need more info or help to run this code.\n",
    "# This code is released under GNU General Public License v3.0. Feel free to use it as you wish.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pyexcel_ods3\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_i = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "TEXT_FILES_DIRECTORY = \"./Analysis/Text_files\"\n",
    "ASSESSMENT_FRAMEWORK_DIRECTORY = \"./Analysis/Input/Assessment_framework.ods\"\n",
    "OUTPUT_DIRECTORY = \"./Analysis/Output\"\n",
    "TEXT_CLEANING = True\n",
    "SENTIMENT_ANALYSIS = False\n",
    "AGGREGATE_VARIABLES = True\n",
    "EXPORT_SENTENCE_LEVEL_DATA = True\n",
    "EXPORT_DOCUMENT_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TEXT FILES\n",
    "def txt_to_dataframe():\n",
    "    file_path = []\n",
    "    for file in os.listdir(TEXT_FILES_DIRECTORY):\n",
    "        file_path.append(os.path.join(TEXT_FILES_DIRECTORY, file))\n",
    "    file_name = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "    data = {}\n",
    "    for file in file_path:\n",
    "        key = file_name.search(file)\n",
    "        with open(file, \"r\", encoding='Latin-1') as read_file:\n",
    "            if key is not None:\n",
    "                data[key[1]] = [read_file.read()]\n",
    "    df = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'document', 0:'text'})\n",
    "    codebook = df[['document']].copy()\n",
    "    codebook_sentiment = codebook\n",
    "    df.head(3)\n",
    "    return df\n",
    "\n",
    "# IMPORT ASSESSMENT FRAMEWORK\n",
    "def create_dataframes(ods_file):\n",
    "    ods = pyexcel_ods3.get_data(ods_file)\n",
    "    variables = process_dataframe(ods['variables'])\n",
    "    variables['variable number'] = variables['variable number'].astype(int)\n",
    "    set_search_strings = process_dataframe(ods['search_strings'])\n",
    "    set_co_occurrences = process_dataframe(ods['co_occurrences'])\n",
    "    set_doc_conditionals = process_dataframe(ods['doc_conditionals'])\n",
    "    set_keywords = process_dataframe(ods['taxonomy'], drop_na=False)\n",
    "    return variables, set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords\n",
    "\n",
    "# Helper function to process the data from each sheet in the ODS file\n",
    "def process_dataframe(sheet_data, lowercase_columns=True, drop_na=True):\n",
    "    df = pd.DataFrame(sheet_data[1:], columns=sheet_data[0])\n",
    "    if lowercase_columns:\n",
    "        df.columns = [convert_to_lowercase(col) for col in df.columns]\n",
    "    df = df.applymap(convert_to_lowercase)\n",
    "    if drop_na:\n",
    "        df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Helper function to convert strings to lowercase\n",
    "def convert_to_lowercase(l):\n",
    "    if isinstance(l, str):\n",
    "        return l.lower().strip()\n",
    "    return l\n",
    "\n",
    "# ORGANIZE KEYWORDS IN DICTIONARY\n",
    "def organize_keywords(df):\n",
    "    cols = df.columns\n",
    "    key_dict = {}\n",
    "    for col in cols:\n",
    "        # Filter out empty strings and strings consisting only of whitespace\n",
    "        values = [str(value).strip() for value in df[col].dropna() if str(value).strip()]\n",
    "        key_dict[col.lower()] = values\n",
    "    return key_dict\n",
    "\n",
    "# CLEAN TEXT\n",
    "def clean_text(df, content):\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'(\\d+)$', r'\\1.', text, flags=re.MULTILINE))\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE))\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\.{2,}', '.', text))\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\n\\s*\\n', '\\n', text).strip())\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\n(?=[a-z])', ' ', text))\n",
    "\n",
    "# SPLIT TEXT INTO SENTENCES\n",
    "def split_sentences(df): \n",
    "    df[\"sentences\"] = df[\"text\"].apply(nltk.sent_tokenize)\n",
    "    df[\"sentences\"] = df[\"sentences\"].apply(lambda sentences: [sentence.lower() for sentence in sentences])\n",
    "    return df.explode(\"sentences\")\n",
    "\n",
    "# LABEL SENTENCES BASED ON KEYWORDS IN TAXONOMY\n",
    "def check_groups(row, word_dict, *args): \n",
    "    for key, words in word_dict.items():\n",
    "        for word in words:\n",
    "            pattern = r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\")\n",
    "            matches = re.findall(pattern, row['sentences'])\n",
    "            if len(matches) > 0:\n",
    "                row[key] = True\n",
    "                break\n",
    "        else:\n",
    "            row[key] = False\n",
    "    return row\n",
    "\n",
    "# LABEL SENTENCES BASED ON CO-OCCURRENCES\n",
    "def initiate_co_occurrences(df, set_co_occurrences, word_dict):\n",
    "    # Convert the 'name' column to the 'object' data type\n",
    "    set_co_occurrences['name of co-occurrence'] = set_co_occurrences['name of co-occurrence'].astype(object)\n",
    "    for index, row in set_co_occurrences.iterrows():\n",
    "        # Replace NaN values in the 'name' column with an empty string\n",
    "        if pd.isnull(row['name of co-occurrence']):\n",
    "            row['name of co-occurrence'] = ''\n",
    "        key1 = row['first list']\n",
    "        key2 = row['second list']\n",
    "        distance = row['distance between lists']\n",
    "        name = row['name of co-occurrence']\n",
    "        df = df.apply(find_co_occurrences, key1=key1, key2=key2, distance=distance, name=name, word_dict=word_dict, axis=1)\n",
    "    return df\n",
    "\n",
    "# helper function to find co-occurrences\n",
    "def find_co_occurrences(row, key1, key2, word_dict, distance, name):\n",
    "    words1 = word_dict[key1]\n",
    "    words2 = word_dict[key2]\n",
    "    words1 = [str(word) for word in words1]\n",
    "    words2 = [str(word) for word in words2]\n",
    "    patterns1 = [r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\") for word in words1]\n",
    "    patterns2 = [r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\") for word in words2]\n",
    "    occurrences1 = []\n",
    "    occurrences2 = []\n",
    "    for pattern in patterns1:\n",
    "        occurrences1 += [m.start() for m in re.finditer(pattern, row['sentences'])]\n",
    "    for pattern in patterns2:\n",
    "        occurrences2 += [m.start() for m in re.finditer(pattern, row['sentences'])]\n",
    "    co_occurrences = []\n",
    "    for occ1 in occurrences1:\n",
    "        for occ2 in occurrences2:\n",
    "            start = min(occ1, occ2)\n",
    "            end = max(occ1, occ2)\n",
    "            num_words = len(row['sentences'][start:end].split())\n",
    "            co_occurrences.append(num_words)\n",
    "    if co_occurrences:\n",
    "        if min(co_occurrences) <= distance:\n",
    "            row[name] = True \n",
    "        else:\n",
    "            row[name] = False\n",
    "    else:\n",
    "        row[name] = False\n",
    "    return row\n",
    "\n",
    "# INITIATE DOCUMENT CONDITIONALS    \n",
    "def initiate_document_conditionals(df, set_doc_conditionals):\n",
    "    set_doc_conditionals.apply(lambda row: find_document_conditionals(df, row['name of document-level conditional'], row['list']), axis=1)\n",
    "\n",
    "# Helper function to find document conditionals\n",
    "def find_document_conditionals(df, name, conditional): \n",
    "    df[name] = df['document'].map(df.groupby('document').apply(lambda x: x[conditional].eq(1).any()))\n",
    "\n",
    "# SENTIMENT ANALYSIS\n",
    "def vadar_sentiment_analysis(text):\n",
    "        return sent_i.polarity_scores(text)['compound']\n",
    "\n",
    "# RUN QUERIES\n",
    "def run_queries(df, set_search_strings):\n",
    "    results = df.copy()\n",
    "    for _, row in set_search_strings.iterrows():\n",
    "        variable, query = row['proxy'], row['query']\n",
    "        # Convert AND, OR, NOT operations to their Python equivalents\n",
    "        query = query.replace(\"AND\", \"and\").replace(\"OR\", \"or\").replace(\"NOT\", \"not\")\n",
    "        # Evaluate the query for each row in df\n",
    "        results[variable] = df.apply(lambda x: evaluate_query(x, query), axis=1)\n",
    "    return results\n",
    "\n",
    "# Helper function to evaluate the query\n",
    "def evaluate_query(row, query):\n",
    "    for column in row.index:\n",
    "        query = query.replace(f'\"{column}\"', str(row[column]))\n",
    "    try:\n",
    "        return eval(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating query: {query} - {e}\")\n",
    "        return False\n",
    "\n",
    "# AGGREGATE VARIABLES\n",
    "   \n",
    "def aggregate_and_rename_variables(results, variables):\n",
    "    for _, row in variables.iterrows():\n",
    "        condition, threshold_str = row['aggregation query'].split('>')\n",
    "        threshold = int(threshold_str.strip())   \n",
    "        child_vars = []\n",
    "        for var in condition.split('+'):\n",
    "            var_name = var.strip(\" ()\\\"'\")\n",
    "            if var_name in results.columns:\n",
    "                child_vars.append(var_name)\n",
    "        parent_name = f\"{row['variable number']}. {row['variable']}\"\n",
    "        count_true = sum([results[var] for var in child_vars])\n",
    "        results[parent_name] = count_true > threshold \n",
    "        for i, var_name in enumerate(child_vars, start=1):\n",
    "            new_name = f\"{row['variable number']}.{i}. {var_name}\"\n",
    "            results.rename(columns={var_name: new_name}, inplace=True)\n",
    "    return results\n",
    "\n",
    "# EXPORT RESULTS\n",
    "def results_document(df):\n",
    "    v_list = [col for col in df.columns if bool(re.match('^[0-9]+\\.[0-9]*', str(col)))]\n",
    "    codebook = df.groupby(['document'])[v_list].sum().astype(int).reset_index()\n",
    "    return codebook\n",
    "\n",
    "def results_document_clipped(df):\n",
    "    v_list = [col for col in df.columns if bool(re.match('^[0-9]+\\.[0-9]*', str(col)))]\n",
    "    code_bool = df.groupby(['document'])[v_list].sum().reset_index()\n",
    "    code_bool[v_list] = code_bool[v_list].clip(upper=1)\n",
    "    return code_bool\n",
    "\n",
    "def results_document_percentage_of_sentences(df):\n",
    "    v_list = [col for col in df.columns if bool(re.match('^[0-9]+\\.[0-9]*', str(col)))]\n",
    "    codebook_count_sent = df.groupby(['document'])['sentences'].count().reset_index()\n",
    "    code_sent_percent = df.groupby(['document'])[v_list].sum().reset_index()\n",
    "    code_sent_percent = code_sent_percent.merge(codebook_count_sent, on='document', how='outer')\n",
    "    code_sent_percent[v_list] = code_sent_percent[v_list].div(code_sent_percent['sentences'], axis=0).multiply(100)\n",
    "    code_sent_percent.drop('sentences', axis=1, inplace=True)\n",
    "    return code_sent_percent\n",
    "\n",
    "def results_document_sentiment(df):\n",
    "    v_list = [col for col in df.columns if bool(re.match('^[0-9]+\\.[0-9]*', str(col)))]\n",
    "    codebook_sentiment = df[['document']].drop_duplicates()\n",
    "    for var in v_list:\n",
    "        sentiment = df[df[var]].groupby('document', as_index=False)['vadar_compound'].mean()\n",
    "        codebook_sentiment[var] = codebook_sentiment['document'].map(sentiment.set_index('document')['vadar_compound'])\n",
    "    return codebook_sentiment.groupby('document').mean().reset_index()\n",
    "\n",
    "\n",
    "# CHECK FOR USER-MADE ERRORS IN ASSESSEMENT FRAMEWORK\n",
    "def preliminary_checks(set_search_strings, set_doc_conditionals, set_co_occurrences, taxonomy):\n",
    "    if not os.path.exists(ASSESSMENT_FRAMEWORK_DIRECTORY):\n",
    "        print(f\"Error: The Assessment framework file '{ASSESSMENT_FRAMEWORK_DIRECTORY}' not found.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Check the search strings for errors\n",
    "def check_search_strings(search_strings, taxonomy, doc_conditionals, co_occurrences,variables):\n",
    "    required_columns = ['proxy', 'query']\n",
    "    if not all(column in search_strings.columns for column in required_columns):\n",
    "        print(f\"Error: The search_strings tab does not have the required columns. Keep the column headers included in the template.\")\n",
    "        return False\n",
    "\n",
    "    if search_strings.isnull().values.any():\n",
    "        print(\"Error: The search_strings tab contains NaN values.\")\n",
    "        return False\n",
    "\n",
    "    valid_terms = set(taxonomy.columns) | set(doc_conditionals['name of document-level conditional']) | set(co_occurrences['name of co-occurrence'])\n",
    "    \n",
    "    if search_strings['proxy'].duplicated().any():\n",
    "        print(\"Error: The 'variable' column in the search_strings tab contains duplicate entries.\")\n",
    "        return False\n",
    "    \n",
    "    for query in search_strings['query']:\n",
    "        if query.count(\"(\") != query.count(\")\"):\n",
    "            print(f\"Error: Mismatched parentheses in '{query}'.\")\n",
    "            return False\n",
    "        \n",
    "        strings_in_quotes = re.findall(r'\"([^\"]*)\"', query)\n",
    "        for string in strings_in_quotes:\n",
    "            if string not in valid_terms:\n",
    "                print(f\"Error: The string '{string}' inside quotation marks is not found in the provided taxonomy, document-level conditionals, or co-occurrences.\")\n",
    "                return False\n",
    "            \n",
    "        query_simplified = re.sub(r'\"[^\"]*\"', '', query)\n",
    "        query_simplified = re.sub(r'\\b(and|or|not)\\b', '', query_simplified, flags=re.IGNORECASE)\n",
    "        query_simplified = query_simplified.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        \n",
    "        if query_simplified.strip() != \"\":\n",
    "            print(f\"Error: Query contains invalid or improperly quoted strings: {query_simplified.strip()}\")\n",
    "            return False\n",
    "\n",
    "        if '\"\"' in query or re.search(r'\"\\s+\"', query):\n",
    "            print(\"Error: Found strings not properly enclosed in quotation marks or missing quotation marks.\")\n",
    "            return False\n",
    "\n",
    "    required_co_occurrence_columns = ['name of co-occurrence', 'first list', 'distance between lists', 'second list']\n",
    "    if not all(column in co_occurrences.columns for column in required_co_occurrence_columns):\n",
    "        print(\"Error: co_occurrences tab does not have the required columns.\")\n",
    "        return False\n",
    "    \n",
    "    if co_occurrences.isnull().values.any():\n",
    "        print(\"Error: co_occurrences tab contains NaN values.\")\n",
    "        return False\n",
    "    \n",
    "    valid_terms_for_co_occurrences = set(taxonomy.columns)\n",
    "    for index, row in co_occurrences.iterrows():\n",
    "        if row['first list'] not in valid_terms_for_co_occurrences or row['second list'] not in valid_terms_for_co_occurrences:\n",
    "            print(f\"Error: Check if these strings in co_occurrences '{row['first list']}' or '{row['second list']}' are present in the taxonomy as column headers.\")\n",
    "            return False\n",
    "\n",
    "    # Check doc_conditionals\n",
    "    required_doc_conditional_columns = ['name of document-level conditional', 'list']\n",
    "    if not all(column in doc_conditionals.columns for column in required_doc_conditional_columns):\n",
    "        print(\"Error: doc_conditionals tab does not have the required columns.\")\n",
    "        return False\n",
    "    \n",
    "    if doc_conditionals.isnull().values.any():\n",
    "        print(\"Error: doc_conditionals tab contains NaN values.\")\n",
    "        return False\n",
    "    \n",
    "    for index, row in doc_conditionals.iterrows():\n",
    "        if row['list'] not in valid_terms_for_co_occurrences:\n",
    "            print(f\"Error: The string '{row['list']}' in doc_conditionals is not present in the taxonomy as column headers.\")\n",
    "            return False\n",
    "        \n",
    "    if AGGREGATE_VARIABLES:\n",
    "        if variables.isnull().values.any():\n",
    "            print(\"Error: The variables DataFrame contains NaN values.\")\n",
    "            return False\n",
    "\n",
    "        all_variables_in_search_strings = set(search_strings['proxy'])\n",
    "\n",
    "        child_variables = set()\n",
    "        for query in variables['aggregation query']:\n",
    "            matches = re.findall(r'\"(.*?)\"', query)\n",
    "            child_variables.update(matches)\n",
    "\n",
    "        missing_variables = child_variables - all_variables_in_search_strings\n",
    "        if missing_variables:\n",
    "            print(f\"Error: The following proxies are not present in the search_strings 'proxy' column: {missing_variables}\")\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():  \n",
    "    # Load, clean, and organize data\n",
    "    df = txt_to_dataframe()\n",
    "    variables, set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords = create_dataframes(ASSESSMENT_FRAMEWORK_DIRECTORY)\n",
    "    # check for errors in the assessment framework\n",
    "    if not preliminary_checks(set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords):\n",
    "        print(\"Preliminary checks failed. Exiting...\")\n",
    "        return  \n",
    "    if not check_search_strings(set_search_strings, set_keywords, set_doc_conditionals, set_co_occurrences, variables):\n",
    "        print(\"Search strings check failed.\")\n",
    "        return\n",
    "    # Clean text and split into sentences\n",
    "    if TEXT_CLEANING:\n",
    "        clean_text(df, 'text')\n",
    "    df = split_sentences(df)\n",
    "    df = df.drop('text', axis=1)\n",
    "    key_dict = organize_keywords(set_keywords)\n",
    "    # Analyze text based on the taxonomy\n",
    "    df = df.apply(check_groups, axis=1, args=(key_dict,))\n",
    "    df = initiate_co_occurrences(df, set_co_occurrences, key_dict)\n",
    "    initiate_document_conditionals(df, set_doc_conditionals)\n",
    "    if SENTIMENT_ANALYSIS:\n",
    "        df['vadar_compound'] = df['sentences'].apply(vadar_sentiment_analysis) \n",
    "    # Run queries and aggregate variables\n",
    "    results = run_queries(df, set_search_strings)\n",
    "    if AGGREGATE_VARIABLES:\n",
    "        results = aggregate_and_rename_variables(results, variables)\n",
    "    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "    if EXPORT_DOCUMENT_DATA:\n",
    "        codebook = results_document(results)\n",
    "        codebook.to_csv(f'{OUTPUT_DIRECTORY}/results_document.tsv', sep='\\t', index=False)\n",
    "        code_bool = results_document_clipped(results)\n",
    "        code_bool.to_csv(f'{OUTPUT_DIRECTORY}/results_document_clipped.tsv', sep='\\t', index=False)    \n",
    "        code_sent_percent = results_document_percentage_of_sentences(results)\n",
    "        code_sent_percent.to_csv(f'{OUTPUT_DIRECTORY}/results_document_percentage_of_sentences.tsv', sep='\\t', index=False)\n",
    "        if SENTIMENT_ANALYSIS:\n",
    "            codebook_sentiment = results_document_sentiment(results)\n",
    "            codebook_sentiment.to_csv(f'{OUTPUT_DIRECTORY}/results_document_sentiment.tsv', sep='\\t', index=False)\n",
    "    if EXPORT_SENTENCE_LEVEL_DATA:\n",
    "        results.to_csv(f'{OUTPUT_DIRECTORY}/results_sentences.tsv', sep='\\t', index=False)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df6a8c26971e94b7f8e04ae2b63413e05f265476cd0a3206034812b2c9bc52e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
