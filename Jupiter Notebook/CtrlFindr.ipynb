{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MPC_CS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MPC_CS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_i = SentimentIntensityAnalyzer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT FILES\n",
    "def txt_to_dataframe():\n",
    "    file_path = []\n",
    "    for file in os.listdir(\"./TXT\"): # ADJUST PATH OF DRIECTORY\n",
    "        file_path.append(os.path.join(\"./TXT\", file)) # ADJUST PATH OF DRIECTORY Articles_English_txt  Policies_txt\n",
    "    file_name = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "    data = {}\n",
    "    for file in file_path:\n",
    "        key = file_name.search(file)\n",
    "        with open(file, \"r\", encoding='Latin-1') as read_file:\n",
    "            if key is not None:\n",
    "                data[key[1]] = [read_file.read()]\n",
    "    df = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'document', 0:'text'})\n",
    "    codebook = df[['document']].copy()\n",
    "    codebook_sentiment = codebook\n",
    "    df.head(3)\n",
    "    return df\n",
    "\n",
    "# Lowercase\n",
    "def convert_to_lowercase(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.lower()\n",
    "    return x\n",
    "\n",
    "## IMPORT search_strings, co_occurrences, doc_conditionals, keywords\n",
    "def create_dataframes(xlsx_file):\n",
    "    xlsx = pd.ExcelFile(xlsx_file)\n",
    "    # Load each sheet into a separate dataframe\n",
    "    set_search_strings = pd.read_excel(xlsx, sheet_name='set_search_strings').applymap(convert_to_lowercase)\n",
    "    set_co_occurrences = pd.read_excel(xlsx, sheet_name='set_co_occurrences').applymap(convert_to_lowercase)\n",
    "    set_doc_conditionals = pd.read_excel(xlsx, sheet_name='set_doc_conditionals').applymap(convert_to_lowercase)\n",
    "    set_keywords = pd.read_excel(xlsx, sheet_name='set_keywords').applymap(convert_to_lowercase)\n",
    "    # Return the dataframes as a tuple\n",
    "    return set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords \n",
    "\n",
    "\n",
    "## ORGANIZE THE TAXONOMY INTO A DICTIONARY\n",
    "def organize_keywords(df):\n",
    "    cols = df.columns\n",
    "    key_dict = {}\n",
    "    for col in cols:\n",
    "        values = [str(value) for value in df[col].dropna().tolist()]\n",
    "        #values = df[col].dropna().tolist()############\n",
    "        key_dict[col.lower()] = values\n",
    "    return key_dict\n",
    "\n",
    "## CLEAN TEXT and SPLIT SENTENCES\n",
    "def clean_text(df, content): # Lowercase, clean and strip text.\n",
    "    df[content] = df[content].apply(lambda text: text.lower())\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\s+', ' ', text).strip())\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE))\n",
    "    \n",
    "def remove_stopwords(df, content, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    df[content] = df[content].apply(lambda text: \" \".join([word for word in nltk.word_tokenize(text) if word not in stop_words]))\n",
    "\n",
    "def split_sentences(df): # Splits text in a list of sentences\n",
    "    df[\"sentences\"] = df[\"text\"].apply(nltk.sent_tokenize)\n",
    "    return df.explode(\"sentences\")\n",
    "\n",
    "## OPTIONAL FUNCTION THAT CAN BE USED TO DROP SENTENCES THAT INCLUDE SPECIFIC STRINGS\n",
    "def drop_selected_sentence(df):\n",
    "    mask = df['sentences'].str.contains('conflict of interest|conflicts of interest', case=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "## CHECK OCCURRENCE OF GROUPS OF KEYWORDS\n",
    "def check_groups(row, word_dict, *args): # Check if any of the words in a dictionary of lists are present in a given string as an exact match.\n",
    "    for key, words in word_dict.items():\n",
    "        for word in words:\n",
    "            # Replace * character with '\\w*'\n",
    "            pattern = r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\")\n",
    "            # Find all the occurrences of the pattern in the string\n",
    "            matches = re.findall(pattern, row['sentences'])\n",
    "            if len(matches) > 0:\n",
    "                row[key] = True\n",
    "                break\n",
    "        else:\n",
    "            row[key] = False\n",
    "    return row\n",
    "\n",
    "\n",
    "def check_words(row, word_dict):\n",
    "    # Iterate over the keys and values in the dictionary\n",
    "    for key, words in word_dict.items():\n",
    "        for word in words:\n",
    "            pattern = r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\")\n",
    "            matches = re.findall(pattern, row['sentences'])\n",
    "            if len(matches) > 0:\n",
    "                row[word] = True\n",
    "            else:\n",
    "                row[word] = False\n",
    "    return row\n",
    "\n",
    "## FIND CO-OCCURRENCES\n",
    "def find_co_occurrences(row, key1, key2, word_dict, distance, name):\n",
    "    # Get the lists of words for the two keys\n",
    "    words1 = word_dict[key1]\n",
    "    words2 = word_dict[key2]\n",
    "    \n",
    "    # Convert the dictionary values to strings\n",
    "    words1 = [str(word) for word in words1]\n",
    "    words2 = [str(word) for word in words2]\n",
    "    \n",
    "    # Create regular expression patterns for the two lists of words\n",
    "    patterns1 = [r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\") for word in words1]\n",
    "    patterns2 = [r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\") for word in words2]\n",
    "\n",
    "    # Find the occurrences of the patterns in the text column\n",
    "    occurrences1 = []\n",
    "    occurrences2 = []\n",
    "    for pattern in patterns1:\n",
    "        occurrences1 += [m.start() for m in re.finditer(pattern, row['sentences'])]\n",
    "    for pattern in patterns2:\n",
    "        occurrences2 += [m.start() for m in re.finditer(pattern, row['sentences'])]\n",
    "\n",
    "    # Calculate the co-occurrence distances\n",
    "    co_occurrences = []\n",
    "    for occ1 in occurrences1:\n",
    "        for occ2 in occurrences2:\n",
    "            start = min(occ1, occ2)\n",
    "            end = max(occ1, occ2)\n",
    "            num_words = len(row['sentences'][start:end].split())\n",
    "            co_occurrences.append(num_words)\n",
    "\n",
    "    # If there are any co-occurrences, add a new column with the minimum distance\n",
    "    if co_occurrences:\n",
    "        if min(co_occurrences) <= distance:\n",
    "            row[name] = True \n",
    "        else:\n",
    "            row[name] = False\n",
    "    else:\n",
    "        row[name] = np.nan\n",
    "\n",
    "    return row\n",
    "\n",
    "## INITIATE CO-OCCURRENCE ANALYSIS\n",
    "def initiate_co_occurrences(df, set_co_occurrences, word_dict):\n",
    "    # Convert the 'name' column to the 'object' data type\n",
    "    set_co_occurrences['name'] = set_co_occurrences['name'].astype(object)\n",
    "    \n",
    "    for index, row in set_co_occurrences.iterrows():\n",
    "        # Replace NaN values in the 'name' column with an empty string\n",
    "        if pd.isnull(row['name']):\n",
    "            row['name'] = ''\n",
    "        \n",
    "        key1 = row['group1']\n",
    "        key2 = row['group2']\n",
    "        distance = row['distance']\n",
    "        name = row['name']\n",
    "        \n",
    "        df = df.apply(find_co_occurrences, key1=key1, key2=key2, distance=distance, name=name, word_dict=word_dict, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def exclude_documents(df, key):\n",
    "    # Check if the keyword group is present in each sentence\n",
    "    df = df.apply(check_groups, axis=1, word_dict={key: key_dict[key]})\n",
    "    \n",
    "    # Determine if each document has any sentences with a true value for the keyword group\n",
    "    find_document_conditionals(df, f\"{key}_conditional\", key)\n",
    "    \n",
    "    # Filter the dataframe to exclude documents that do not meet the condition\n",
    "    df = df[df[f\"{key}_conditional\"] == True].reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_document_conditionals(df, name, conditional): \n",
    "    df[name] = df['document'].map(df.groupby('document').apply(lambda x: x[conditional].eq(1).any()))\n",
    "    \n",
    "def initiate_document_conditionals(df, set_doc_conditionals):\n",
    "    set_doc_conditionals.apply(lambda row: find_document_conditionals(df, row['name'], row['group']), axis=1)\n",
    "\n",
    "def vadar_sentiment_analysis(text):\n",
    "        return sent_i.polarity_scores(text)['compound']\n",
    "\n",
    "## APPLY SEARCH STRINGS G2V\n",
    "def group_to_variable(df, variable_number, group_1):\n",
    "    for row in df:\n",
    "        name = float(variable_number)\n",
    "        condition = group_1\n",
    "        df[name] = (df[condition] == True)\n",
    "\n",
    "## APPLY SEARCH STRINGS B2V\n",
    "def bool_to_variable(df, parent_variable_number, variable_number, *args):\n",
    "    name = variable_number\n",
    "    con1, operator_1, con2 = args[:3]\n",
    "    if len(args) == 3:\n",
    "        if operator_1 == 'and':\n",
    "            df[name] = (df[con1] == True) & (df[con2] == True)\n",
    "        elif operator_1 == 'or':\n",
    "            df[name] = (df[con1] == True) | (df[con2] == True)\n",
    "    else:\n",
    "        operator_2, con3 = args[3:]\n",
    "        if operator_1 == 'and' and operator_2 == 'and':\n",
    "            df[name] = (df[con1] == True) & (df[con2] == True) & (df[con3] == True)\n",
    "        elif operator_1 == 'and' and operator_2 == 'or':\n",
    "            df[name] = (df[con1] == True) & ((df[con2] == True) | (df[con3] == True))\n",
    "        elif operator_1 == 'or' and operator_2 == 'and':\n",
    "            df[name] = (df[con1] == True) | ((df[con2] == True) & (df[con3] == True))\n",
    "        elif operator_1 == 'or' and operator_2 == 'or':\n",
    "            df[name] = (df[con1] == True) | ((df[con2] == True) | (df[con3] == True))\n",
    "\n",
    "## AGGREGATE RESULTS\n",
    "def aggregate_variables(prev_pvn, df):\n",
    "    df[prev_pvn] = False\n",
    "    var_nums = set_search_strings[set_search_strings['parent_variable_number'] == prev_pvn]['variable_number'].tolist()\n",
    "    df[prev_pvn] = df[var_nums].any(axis=1)\n",
    "    \n",
    "\n",
    "## INITIATE SEARCH STRINGS\n",
    "def initiate_search_strings(search_strings, df):\n",
    "    prev_pvn = None\n",
    "    for index, row in search_strings.iterrows():\n",
    "        if row['parent_variable_number'] != prev_pvn:\n",
    "            if prev_pvn is not None:\n",
    "                aggregate_variables(prev_pvn, df)\n",
    "            prev_pvn = row['parent_variable_number']\n",
    "        if row['search_type'] == 'g2v':\n",
    "            group_1 = convert_to_appropriate_type(row['group_1'])\n",
    "            group_to_variable(df, row['variable_number'], group_1)\n",
    "        elif row['search_type'] == 'b2v':\n",
    "            group_1 = convert_to_appropriate_type(row['group_1'])\n",
    "            group_2 = convert_to_appropriate_type(row['group_2'])\n",
    "            if pd.isnull(row['operator_2']):\n",
    "                bool_to_variable(df, row['parent_variable_number'], row['variable_number'], group_1, row['operator_1'], group_2)\n",
    "            else:\n",
    "                group_3 = convert_to_appropriate_type(row['group_3'])\n",
    "                bool_to_variable(df, row['parent_variable_number'], row['variable_number'], group_1, row['operator_1'], group_2, row['operator_2'], group_3)\n",
    "    if prev_pvn is not None:\n",
    "        aggregate_variables(prev_pvn, df)   \n",
    "\n",
    "\n",
    "def convert_to_appropriate_type(value):\n",
    "    if isinstance(value, int):\n",
    "        return float(value)\n",
    "    elif isinstance(value, float):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "# CREATE TSV FILES WITH THE RESULTS\n",
    "def create_codebook(df):\n",
    "    v_list = []\n",
    "    for col in df.columns:\n",
    "        if bool(re.match('^[0-9]+(\\.[0-9]+)?$', str(col))):\n",
    "            v_list.append(col)\n",
    "            v_list.sort()\n",
    "    # codebook of results\n",
    "    codebook = df.groupby(['document'])[v_list].sum().astype(int).reset_index()\n",
    "    # codebook of results but clipped to 1\n",
    "    code_bool = codebook.copy()\n",
    "    code_bool[v_list] = code_bool[v_list].clip(upper=1)\n",
    "    # codebook of results as percentage of total no. of sentences per document\n",
    "    codebook_count_sent = df.groupby(['document'])['sentences'].count().astype(int).reset_index()\n",
    "    code_sent_percent = codebook.merge(codebook_count_sent, on='document', how='outer')\n",
    "    code_sent_percent[v_list]=code_sent_percent[v_list].div(code_sent_percent['sentences'], axis=0)\n",
    "    code_sent_percent[v_list]=code_sent_percent[v_list].multiply(100)\n",
    "    code_sent_percent.drop('sentences', axis=1, inplace=True)\n",
    "    # codebook of sentiment\n",
    "    codebook_sentiment = df[['document']]\n",
    "    for var in v_list:\n",
    "        sentiment = df[df[var]].groupby('document', as_index=False).vadar_compound.mean()\n",
    "        codebook_sentiment = codebook_sentiment.copy()\n",
    "        codebook_sentiment.loc[:, var] = codebook_sentiment['document'].map(sentiment.set_index('document')['vadar_compound'])\n",
    "        #codebook_sentiment[var] = codebook_sentiment['document'].map(sentiment.set_index('document')['vadar_compound'])\n",
    "    codebook_sentiment = codebook_sentiment.groupby('document').mean().reset_index()\n",
    "    save_codebook(codebook, code_sent_percent, code_bool, codebook_sentiment)\n",
    "\n",
    "def save_codebook(codebook, code_percent, code_bool, codebook_sentiment):\n",
    "    dfs = {'codebook':codebook, 'code_percent':code_percent, 'codebook_bool':code_bool, 'codebook_sentiment': codebook_sentiment}\n",
    "    for sheet_name in dfs.keys():\n",
    "        dfs[sheet_name].to_csv(f'{sheet_name}.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = txt_to_dataframe()\n",
    "set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords = create_dataframes('Assessment_framework.xlsx') # ADJUST PATH OF DRIECTORY\n",
    "clean_text(df,'text')\n",
    "df = split_sentences(df)\n",
    "df = df.drop('text', axis=1)\n",
    "key_dict = organize_keywords(set_keywords)\n",
    "\n",
    "#df = drop_selected_sentence(df) ## OPTIONAL STEP to drop sentences with specific strings\n",
    "#key_to_exclude = \"climatechange\"  # Replace with the key you want to use for exclusion ## OPTIONAL STEP\n",
    "#df = exclude_documents(df, key_to_exclude) ## OPTIONAL STEP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[con1] == True) & ((df[con2] == True) | (df[con3] == True))\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[prev_pvn] = False\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[con1] == True) & (df[con2] == True)\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[con1] == True) & (df[con2] == True)\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[prev_pvn] = False\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[condition] == True)\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[condition] == True)\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[prev_pvn] = False\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name] = (df[condition] == True)\n",
      "C:\\Users\\MPC_CS\\AppData\\Local\\Temp\\ipykernel_9084\\3414642345.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[prev_pvn] = False\n"
     ]
    }
   ],
   "source": [
    "df = df.apply(check_groups, axis=1, args=(key_dict,))\n",
    "#df = df.apply(check_words, word_dict=key_dict, axis=1) ## OPTIONAL STEP add a columns for each individual lists from the taxonomy. \n",
    "df = initiate_co_occurrences(df, set_co_occurrences, key_dict)\n",
    "initiate_document_conditionals(df, set_doc_conditionals)\n",
    "df['vadar_compound'] = df['sentences'].apply(vadar_sentiment_analysis) \n",
    "#print(\"Columns in df before calling initiate_search_strings: \", df.columns)  # Add this line\n",
    "initiate_search_strings(set_search_strings, df)\n",
    "df.to_csv('sentences.tsv', sep='\\t', index=False) # OPTIONAL STEP to generate a tsv file containing the codebook with the list of sentences. \n",
    "create_codebook(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df6a8c26971e94b7f8e04ae2b63413e05f265476cd0a3206034812b2c9bc52e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
