{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\MSI-\n",
      "[nltk_data]     CMS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\MSI-\n",
      "[nltk_data]     CMS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For additional information or assistance with running this code, please reach out to contact [at] scartozzi [dot] eu.\n",
    "# This code is released under GNU General Public License v3.0. Feel free to use it as you wish.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent_i = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL VARIABLES\n",
    "# specify the directory where the text files are stored:\n",
    "TEXT_FILES_DIRECTORY = \"./Analysis/Text_files\"\n",
    "# specify if you want to load a SQL database or not:\n",
    "LOAD_DATABSE = False\n",
    "# specify the path to the SQL database:\n",
    "DATABASE_PATH = \"./Analysis/Output/database\"\n",
    "# specify the name of the table in the SQL database:\n",
    "TABLE_NAME = 'documents'\n",
    "# specify the directory where the assessment framework is stored:\n",
    "ASSESSMENT_FRAMEWORK_DIRECTORY = \"./Analysis/Input/Assessment_framework.ods\"\n",
    "# specify the directory where the results will be saved:\n",
    "OUTPUT_DIRECTORY = \"./Analysis/Output\"\n",
    "# choose the following settings:\n",
    "TEXT_CLEANING = True\n",
    "SENTIMENT_ANALYSIS = True\n",
    "# choose what to export:\n",
    "EXPORT_SENTENCE_LEVEL_DATA = True\n",
    "EXPORT_DOCUMENT_DATA = True\n",
    "# estimate the time needed to run the code:\n",
    "PRINT_RUN_TIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD AND PREPARE DATA FOR ANALYSIS\n",
    "# IMPORT TXT FILES FROM TEXT_FILES_DIRECTORY\n",
    "def txt_to_sql(directory, db_path):\n",
    "    data = {\"document\": [], \"text\": []}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                data[\"document\"].append(filename.replace(\".txt\", \"\"))\n",
    "                data[\"text\"].append(content)\n",
    "    df = pd.DataFrame(data)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql('documents', conn, if_exists='replace', index=False)\n",
    "    print(df.shape)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "# IMPORT ASSESSMENT FRAMEWORK\n",
    "def create_dataframes(ods_file):\n",
    "    variables = process_dataframe(pd.read_excel(ods_file, sheet_name='variables', engine='odf'))\n",
    "    set_search_strings = process_dataframe(pd.read_excel(ods_file, sheet_name='search_strings', engine='odf'))\n",
    "    set_co_occurrences = process_dataframe(pd.read_excel(ods_file, sheet_name='co_occurrences', engine='odf'))\n",
    "    set_doc_conditionals = process_dataframe(pd.read_excel(ods_file, sheet_name='doc_conditionals', engine='odf'))\n",
    "    set_keywords = process_dataframe(pd.read_excel(ods_file, sheet_name='taxonomy', engine='odf'), drop_na=False)\n",
    "    return variables, set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords\n",
    "\n",
    "# Helper function to process the data from each sheet in the ODS file\n",
    "def process_dataframe(df, lowercase_columns=True, drop_na=False):\n",
    "    if lowercase_columns:\n",
    "        df.columns = [convert_to_lowercase(col) for col in df.columns]\n",
    "    df = df.map(convert_to_lowercase)\n",
    "    if 'query' in df.columns:\n",
    "        df['query'] = df['query'].apply(format_logical_expression)\n",
    "    if drop_na:\n",
    "        df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Helper function to format logical expressions in the search strings' query column\n",
    "def format_logical_expression(expression):\n",
    "    if not isinstance(expression, str):\n",
    "        return expression\n",
    "    expression = re.sub(r\"(?<![a-zA-Z])and(?![a-zA-Z])\", \" and \", expression)\n",
    "    expression = re.sub(r\"(?<![a-zA-Z])or(?![a-zA-Z])\", \" or \", expression)\n",
    "    expression = re.sub(r\"(?<![a-zA-Z])not(?![a-zA-Z])\", \" not \", expression)\n",
    "    expression = re.sub(r\"\\s+\", \" \", expression).strip()\n",
    "    return expression\n",
    "\n",
    "# Helper function to convert strings to lowercase\n",
    "def convert_to_lowercase(l):\n",
    "    if isinstance(l, str):\n",
    "        return l.lower().strip()\n",
    "    return l\n",
    "\n",
    "# ORGANIZE KEYWORDS IN DICTIONARY\n",
    "def organize_keywords(df):\n",
    "    cols = df.columns\n",
    "    key_dict = {}\n",
    "    for col in cols:\n",
    "        values = [str(value).strip() for value in df[col].dropna() if str(value).strip()]\n",
    "        key_dict[col.lower()] = values\n",
    "    return key_dict\n",
    "\n",
    "# CLEAN TEXT DATA\n",
    "def clean_text(df, content):\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE))\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\.{2,}', '.', text))\n",
    "    df[content] = df[content].apply(lambda text: re.sub(r'\\n(?=[a-z])', ' ', text))\n",
    "\n",
    "# SPLIT TEXT INTO SENTENCES\n",
    "def split_sentences(df):\n",
    "    df[\"sentences\"] = df[\"text\"].apply(nltk.sent_tokenize)\n",
    "    return df.explode(\"sentences\")\n",
    "\n",
    "# CLEAN AND DROP SENTENCES\n",
    "def clean_rows(df):\n",
    "    df[\"sentences\"] = df[\"sentences\"].str.lower()\n",
    "    df = df[~df[\"sentences\"].str.match(r'^\\d+$')]                   \n",
    "    df = df[~df[\"sentences\"].str.match(r'^\\d+[\\W_]+$')]             \n",
    "    df = df[~df[\"sentences\"].str.match(r'^[\\W_]+$')]               \n",
    "    df.loc[:, \"sentences\"] = df[\"sentences\"].str.replace('\\n', '', regex=False)\n",
    "    return df\n",
    "\n",
    "## RUN CONTENT ANALYSIS\n",
    "# CHECK PRESENCE OF LISTS FROM TAXONOMY IN SENTENCES\n",
    "def check_groups(df, word_dict):\n",
    "    for key, words in word_dict.items():\n",
    "        df[key] = False\n",
    "        patterns = [r\"\\b{}\\b\".format(re.escape(word).replace(r'\\*', r'\\w*')) for word in words]\n",
    "        combined_pattern = '|'.join(patterns)\n",
    "        df[key] = df['sentences'].str.contains(combined_pattern, regex=True)\n",
    "    return df\n",
    "\n",
    "# CHECK PRESENCE OF CO-OCCURRENCES IN SENTENCES BASED ON check_groups\n",
    "def initiate_co_occurrences(df, set_co_occurrences, word_dict):\n",
    "    set_co_occurrences['name of co-occurrence'] = set_co_occurrences['name of co-occurrence'].astype(object)\n",
    "    compiled_patterns = {}\n",
    "    for key, words in word_dict.items():\n",
    "        patterns = [r\"\\b%s\\b\" % word.replace(\".\", r\"\\.\").replace(\"*\", \"\\w*\") for word in words]\n",
    "        compiled_patterns[key] = re.compile('|'.join(patterns))\n",
    "    for index, row in set_co_occurrences.iterrows():\n",
    "        if pd.isnull(row['name of co-occurrence']):\n",
    "            set_co_occurrences.at[index, 'name of co-occurrence'] = ''\n",
    "        key1 = row['first list']\n",
    "        key2 = row['second list']\n",
    "        distance = row['distance between lists']\n",
    "        name = row['name of co-occurrence']\n",
    "        df[name] = df['sentences'].apply(lambda sentence: find_co_occurrences(sentence, compiled_patterns[key1], compiled_patterns[key2], distance))\n",
    "    return df\n",
    "\n",
    "# Helper function to find co-occurrences\n",
    "def find_co_occurrences(sentence, pattern1, pattern2, distance):\n",
    "    occurrences1 = [m.start() for m in pattern1.finditer(sentence)]\n",
    "    occurrences2 = [m.start() for m in pattern2.finditer(sentence)]\n",
    "    co_occurrences = []\n",
    "    for occ1 in occurrences1:\n",
    "        for occ2 in occurrences2:\n",
    "            start = min(occ1, occ2)\n",
    "            end = max(occ1, occ2)\n",
    "            num_words = len(sentence[start:end].split())\n",
    "            co_occurrences.append(num_words)\n",
    "    return min(co_occurrences) <= distance if co_occurrences else False\n",
    "\n",
    "# CHECK DOCUMENT-LEVEL CONDITIONALS\n",
    "def initiate_document_conditionals(df, set_doc_conditionals):\n",
    "    for _, row in set_doc_conditionals.iterrows():\n",
    "        name = row['name of document-level conditional']\n",
    "        conditional = row['list']\n",
    "        # Use groupby and transform to create a boolean series directly\n",
    "        df[name] = df.groupby('document')[conditional].transform(lambda x: x.any())\n",
    "    return df\n",
    "\n",
    "# SENTIMENT ANALYSIS BASED ON VADAR\n",
    "def vadar_sentiment_analysis(text_series):\n",
    "    # Apply the sentiment analysis to the entire series\n",
    "    sentiment_scores = text_series.apply(lambda text: sent_i.polarity_scores(text)['compound'])\n",
    "    return pd.DataFrame({\n",
    "        'vadar_compound': sentiment_scores,\n",
    "        'sentiment_positive': sentiment_scores > 0,\n",
    "        'sentiment_neutral': sentiment_scores == 0,\n",
    "        'sentiment_neutralpositive': sentiment_scores >= 0,\n",
    "        'sentiment_negative': sentiment_scores < 0,\n",
    "        'sentiment_neutralnegative': sentiment_scores <= 0\n",
    "    })\n",
    "\n",
    "# RUN QUERIES LISTED IN ASSESSMENT_FRAMEWORK\n",
    "def run_queries(df, set_search_strings):\n",
    "    results = df.copy()\n",
    "    for _, row in set_search_strings.iterrows():\n",
    "        variable, query = row['query name'], row['query']\n",
    "        # Convert AND, OR, NOT operations to their Python equivalents\n",
    "        query = query.replace(\"AND\", \"and\").replace(\"OR\", \"or\").replace(\"NOT\", \"not\")\n",
    "        # Evaluate the query for each row in df\n",
    "        results[variable] = df.apply(lambda x: evaluate_query(x, query), axis=1)\n",
    "    return results\n",
    "\n",
    "# Helper function to evaluate the query\n",
    "def evaluate_query(row, query):\n",
    "    for column in row.index:\n",
    "        query = query.replace(f'\"{column}\"', str(row[column]))\n",
    "    try:\n",
    "        return eval(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating query: {query} - {e}\")\n",
    "        return False\n",
    "    \n",
    "# AGGREGATE VARIABLES LISTED IN ASSESSMENT_FRAMEWORK\n",
    "def define_variables(results, variables):\n",
    "    for _, row in variables.iterrows():\n",
    "        condition, threshold_str = row['aggregation query'].split('>')\n",
    "        threshold = int(threshold_str.strip())\n",
    "        child_vars = []\n",
    "        for var in condition.split('+'):\n",
    "            var_name = var.strip(\" ()\\\"'\")\n",
    "            if var_name in results.columns:\n",
    "                child_vars.append(var_name)\n",
    "        variable_number = str(row['variable number'])\n",
    "        if '.' not in variable_number:\n",
    "            variable_number += '.'\n",
    "        parent_name = f\"{variable_number} {row['variable']}\"\n",
    "        count_true = sum([results[var] for var in child_vars])\n",
    "        results[parent_name] = count_true > threshold\n",
    "    return results\n",
    "\n",
    "\n",
    "## EXPORT RESULTS\n",
    "def get_v_list(df):\n",
    "    return [col for col in df.columns if bool(re.match(r'^[0-9]+\\.[0-9]*', str(col)))]\n",
    "\n",
    "def results_document(df, v_list):\n",
    "    codebook = df.groupby('document')[v_list].sum().astype(int).reset_index()\n",
    "    return codebook\n",
    "\n",
    "def results_document_clipped(df, v_list):\n",
    "    code_bool = df.groupby('document')[v_list].sum().clip(upper=1).reset_index()\n",
    "    return code_bool\n",
    "\n",
    "def results_document_percentage_of_sentences(df, v_list):\n",
    "    grouped = df.groupby('document')\n",
    "    codebook_count_sent = grouped['sentences'].count().reset_index()\n",
    "    code_sent_percent = grouped[v_list].sum().reset_index()\n",
    "    code_sent_percent = code_sent_percent.merge(codebook_count_sent, on='document', how='outer')\n",
    "    code_sent_percent[v_list] = code_sent_percent[v_list].div(code_sent_percent['sentences'], axis=0).multiply(100)\n",
    "    code_sent_percent.drop('sentences', axis=1, inplace=True)\n",
    "    return code_sent_percent\n",
    "\n",
    "def results_document_sentiment(df, v_list):\n",
    "    codebook_sentiment = df[['document']].drop_duplicates().set_index('document')\n",
    "    for var in v_list:\n",
    "        sentiment = df.loc[df[var], ['document', 'vadar_compound']].groupby('document')['vadar_compound'].mean()\n",
    "        codebook_sentiment[var] = codebook_sentiment.index.map(sentiment)\n",
    "    return codebook_sentiment.reset_index()\n",
    "\n",
    "### SQL FUNCTIONS MANAGEMENT\n",
    "def load_data_from_sql(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql(f'SELECT * FROM {table_name}', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def save_data_to_sql(df, db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def load_data_from_sql(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql(f'SELECT * FROM {table_name}', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "## CHECK FOR USER-MADE ERRORS IN ASSESSEMENT FRAMEWORK\n",
    "def preliminary_checks(set_search_strings, set_doc_conditionals, set_co_occurrences, taxonomy):\n",
    "    if not os.path.exists(ASSESSMENT_FRAMEWORK_DIRECTORY):\n",
    "        print(f\"Error: The Assessment framework file '{ASSESSMENT_FRAMEWORK_DIRECTORY}' not found.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Check the search strings for errors\n",
    "def check_search_strings(search_strings, taxonomy, doc_conditionals, co_occurrences,variables):\n",
    "    required_columns = ['query name', 'query']\n",
    "    if not all(column in search_strings.columns for column in required_columns):\n",
    "        print(f\"Error: The search_strings tab does not have the required columns. Keep the column headers included in the template.\")\n",
    "        return False\n",
    "    valid_terms = set(taxonomy.columns) | set(doc_conditionals['name of document-level conditional']) | set(co_occurrences['name of co-occurrence'])\n",
    "    if search_strings['query name'].duplicated().any():\n",
    "        print(\"Error: The 'variable' column in the search_strings tab contains duplicate entries.\")\n",
    "        return False\n",
    "    for query in search_strings['query']:\n",
    "        if query.count(\"(\") != query.count(\")\"):\n",
    "            print(f\"Error: Mismatched parentheses in '{query}'.\")\n",
    "            return False\n",
    "        strings_in_quotes = re.findall(r'\"([^\"]*)\"', query)\n",
    "        for string in strings_in_quotes:\n",
    "            if string not in valid_terms and string not in ['sentiment_positive', 'sentiment_neutral', 'sentiment_neutralpositive', 'sentiment_negative', 'sentiment_neutralnegative']:\n",
    "                print(f\"Error: The string '{string}' inside quotation marks is not found in the provided taxonomy, document-level conditionals, or co-occurrences.\")\n",
    "                return False\n",
    "        query_simplified = re.sub(r'\"[^\"]*\"', '', query)\n",
    "        query_simplified = re.sub(r'\\b(and|or|not)\\b', '', query_simplified, flags=re.IGNORECASE)\n",
    "        query_simplified = query_simplified.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        if query_simplified.strip() != \"\":\n",
    "            print(f\"Error: Query contains invalid or improperly quoted strings: {query_simplified.strip()}\")\n",
    "            return False\n",
    "        if '\"\"' in query or re.search(r'\"\\s+\"', query):\n",
    "            print(\"Error: Found strings not properly enclosed in quotation marks or missing quotation marks.\")\n",
    "            return False\n",
    "    required_co_occurrence_columns = ['name of co-occurrence', 'first list', 'distance between lists', 'second list']\n",
    "    if not all(column in co_occurrences.columns for column in required_co_occurrence_columns):\n",
    "        print(\"Error: co_occurrences tab does not have the required columns.\")\n",
    "        return False\n",
    "    valid_terms_for_co_occurrences = set(taxonomy.columns)\n",
    "    for index, row in co_occurrences.iterrows():\n",
    "        if row['first list'] not in valid_terms_for_co_occurrences or row['second list'] not in valid_terms_for_co_occurrences:\n",
    "            print(f\"Error: Check if these strings in co_occurrences '{row['first list']}' or '{row['second list']}' are present in the taxonomy as column headers.\")\n",
    "            return False\n",
    "    required_doc_conditional_columns = ['name of document-level conditional', 'list']\n",
    "    if not all(column in doc_conditionals.columns for column in required_doc_conditional_columns):\n",
    "        print(\"Error: doc_conditionals tab does not have the required columns.\")\n",
    "        return False\n",
    "    for index, row in doc_conditionals.iterrows():\n",
    "        if row['list'] not in valid_terms_for_co_occurrences:\n",
    "            print(f\"Error: The string '{row['list']}' in doc_conditionals is not present in the taxonomy as column headers.\")\n",
    "            return False\n",
    "    all_variables_in_search_strings = set(search_strings['query name'])\n",
    "    child_variables = set()\n",
    "    for query in variables['aggregation query']:\n",
    "        matches = re.findall(r'\"(.*?)\"', query)\n",
    "        child_variables.update(matches)\n",
    "    missing_variables = child_variables - all_variables_in_search_strings\n",
    "    if missing_variables:\n",
    "        print(f\"Error: The following queries are not present in the search_strings 'query name' column: {missing_variables}. Note that query names cannot be numerical\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "## ESTIMATE RUN TIME\n",
    "def estimate_run_time(sample_df, full_run=False): \n",
    "    variables, set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords = create_dataframes(ASSESSMENT_FRAMEWORK_DIRECTORY)\n",
    "    key_dict = organize_keywords(set_keywords)\n",
    "    start_time = time.time()\n",
    "    sample_df = check_groups(sample_df, key_dict)\n",
    "    sample_df = initiate_co_occurrences(sample_df, set_co_occurrences, key_dict)\n",
    "    initiate_document_conditionals(sample_df, set_doc_conditionals)\n",
    "    if SENTIMENT_ANALYSIS:\n",
    "        sentiment_df = vadar_sentiment_analysis(sample_df['sentences'])\n",
    "        sample_df = pd.concat([sample_df, sentiment_df], axis=1)\n",
    "    sample_results = run_queries(sample_df, set_search_strings)\n",
    "    sample_results = define_variables(sample_results, variables)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n",
      "Estimated run time for 1589 sentences: 1.59293470621109 seconds (excluding time to export results).\n"
     ]
    }
   ],
   "source": [
    "def main():  \n",
    "    ## LOAD DATA\n",
    "    if not LOAD_DATABSE:\n",
    "        df = txt_to_sql(TEXT_FILES_DIRECTORY, DATABASE_PATH)\n",
    "    else:\n",
    "        df = load_data_from_sql(DATABASE_PATH, TABLE_NAME) \n",
    "    variables, set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords = create_dataframes(ASSESSMENT_FRAMEWORK_DIRECTORY)\n",
    "    key_dict = organize_keywords(set_keywords)\n",
    "\n",
    "    ## CHECK FOR USER-MADE ERRORS IN ASSESSEMENT FRAMEWORK\n",
    "    if not preliminary_checks(set_search_strings, set_co_occurrences, set_doc_conditionals, set_keywords):\n",
    "        print(\"Preliminary checks failed. Exiting...\")\n",
    "        return  \n",
    "    if not check_search_strings(set_search_strings, set_keywords, set_doc_conditionals, set_co_occurrences, variables):\n",
    "        print(\"Search strings check failed.\")\n",
    "        return\n",
    "    \n",
    "    ## PREPARE DATA\n",
    "    if TEXT_CLEANING:\n",
    "        clean_text(df, 'text')\n",
    "    df = split_sentences(df)\n",
    "    df= clean_rows(df)\n",
    "    df = df.drop('text', axis=1)\n",
    "\n",
    "    ## ESTIMATE RUN TIME\n",
    "    if PRINT_RUN_TIME:\n",
    "        total_sentences = len(df)\n",
    "        if total_sentences < 100:\n",
    "            print(f\"The dataset has only {total_sentences} sentences, which is insufficient to accurately estimate runtime.\")\n",
    "        else:\n",
    "            sample = df.sample(n=100)\n",
    "            time_for_50 = estimate_run_time(sample, full_run=False)\n",
    "            estimated_total_time = (time_for_50 / 100) * total_sentences\n",
    "            print(f\"Estimated run time for {total_sentences} sentences: {estimated_total_time} seconds (excluding time to export results).\")\n",
    "\n",
    "    ## RUN CONTENT ANALYSIS\n",
    "    df = check_groups(df, key_dict)\n",
    "    df = initiate_co_occurrences(df, set_co_occurrences, key_dict)\n",
    "    df = initiate_document_conditionals(df, set_doc_conditionals)\n",
    "    if SENTIMENT_ANALYSIS:\n",
    "        sentiment_df = vadar_sentiment_analysis(df['sentences'])\n",
    "        df = pd.concat([df, sentiment_df], axis=1)\n",
    "    df = run_queries(df, set_search_strings)\n",
    "    df = define_variables(df, variables)\n",
    "\n",
    "    ## EXPORT RESULTS\n",
    "    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "    v_list = get_v_list(df)\n",
    "    if EXPORT_DOCUMENT_DATA:\n",
    "        codebook = results_document(df, v_list)\n",
    "        codebook.to_csv(f'{OUTPUT_DIRECTORY}/results_document.tsv', sep='\\t', index=False)\n",
    "        code_bool = results_document_clipped(df, v_list)\n",
    "        code_bool.to_csv(f'{OUTPUT_DIRECTORY}/results_document_clipped.tsv', sep='\\t', index=False)\n",
    "        code_sent_percent = results_document_percentage_of_sentences(df, v_list)\n",
    "        code_sent_percent.to_csv(f'{OUTPUT_DIRECTORY}/results_document_percentage_of_sentences.tsv', sep='\\t', index=False)\n",
    "        if SENTIMENT_ANALYSIS:\n",
    "            codebook_sentiment = results_document_sentiment(df, v_list)\n",
    "            codebook_sentiment.to_csv(f'{OUTPUT_DIRECTORY}/results_document_sentiment.tsv', sep='\\t', index=False)\n",
    "    if EXPORT_SENTENCE_LEVEL_DATA:\n",
    "        df.to_csv(f'{OUTPUT_DIRECTORY}/results_sentences.tsv', sep='\\t', index=False)\n",
    "    save_data_to_sql(df, DATABASE_PATH, TABLE_NAME)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
